{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37285a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/miniconda3/envs/t5gamma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.39s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/models/AIDC-AI/Ovis2.5-9B', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6\n",
    "    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ovis2_5(\n",
       "  (llm): Qwen3ForCausalLM(\n",
       "    (model): Qwen3Model(\n",
       "      (embed_tokens): Embedding(151936, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen3DecoderLayer(\n",
       "          (self_attn): Qwen3Attention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3MLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "            (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       "  )\n",
       "  (visual_tokenizer): VisualTokenizer(\n",
       "    (vit): Siglip2NavitModel(\n",
       "      (vision_model): Siglip2VisionTransformer(\n",
       "        (embeddings): Siglip2VisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "          (position_embedding): Embedding(1024, 1152)\n",
       "        )\n",
       "        (encoder): Siglip2Encoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x Siglip2EncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): Siglip2Attention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Siglip2MLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=4608, out_features=65532, bias=False)\n",
       "      (1): LayerNorm((65532,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (vte): VisualEmbedding(65536, 4096)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd2387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/models/Qwen/Qwen3-8B-Base\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e0ca2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc4222a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMVTokenizerFast(name_or_path='/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/models/openbmb/MiniCPM-V-4_5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<unk>', '<image>', '</image>', '<ref>', '</ref>', '<box>', '</box>', '<quad>', '</quad>', '<point>', '</point>', '<slice>', '</slice>', '<image_id>', '</image_id>', '<unit>', '</unit>', '<|reserved_0|>', '<|reserved_1|>', '<|reserved_2|>', '<|reserved_3|>', '<|reserved_4|>', '<|reserved_5|>', '<|reserved_6|>', '<|reserved_7|>', '<|reserved_8|>', '<|reserved_9|>', '<|reserved_10|>', '<|reserved_11|>', '<|reserved_12|>', '<|reserved_13|>', '<|reserved_14|>', '<|reserved_15|>', '<|reserved_16|>', '<|reserved_17|>', '<|reserved_18|>', '<|reserved_19|>', '<|reserved_20|>', '<|reserved_21|>', '<|reserved_22|>', '<|reserved_23|>', '<|reserved_24|>', '<|reserved_25|>', '<|reserved_26|>', '<|reserved_27|>', '<|reserved_28|>', '<|reserved_29|>', '<|reserved_30|>', '<|reserved_31|>', '<|reserved_32|>', '<|reserved_33|>', '<|reserved_34|>', '<|reserved_35|>', '<|reserved_36|>', '<|reserved_37|>', '<|reserved_38|>', '<|reserved_39|>', '<|reserved_40|>', '<|reserved_41|>', '<|reserved_42|>', '<|reserved_43|>', '<|reserved_44|>', '<|reserved_45|>', '<|reserved_46|>', '<|reserved_47|>', '<|reserved_48|>', '<|reserved_49|>', '<|reserved_50|>', '<|reserved_51|>', '<|reserved_52|>', '<|reserved_53|>', '<|reserved_54|>', '<|reserved_55|>', '<|reserved_56|>', '<|reserved_57|>', '<|reserved_58|>', '<|reserved_59|>', '<|reserved_60|>', '<|reserved_61|>', '<|reserved_62|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t128244: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151669: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151670: AddedToken(\"</image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151671: AddedToken(\"<ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151672: AddedToken(\"</ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151673: AddedToken(\"<box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151674: AddedToken(\"</box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151675: AddedToken(\"<quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151676: AddedToken(\"</quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151677: AddedToken(\"<point>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151678: AddedToken(\"</point>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151679: AddedToken(\"<slice>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151680: AddedToken(\"</slice>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151681: AddedToken(\"<image_id>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151682: AddedToken(\"</image_id>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151683: AddedToken(\"<unit>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151684: AddedToken(\"</unit>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151685: AddedToken(\"<|reserved_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151686: AddedToken(\"<|reserved_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151687: AddedToken(\"<|reserved_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151688: AddedToken(\"<|reserved_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151689: AddedToken(\"<|reserved_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151690: AddedToken(\"<|reserved_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151691: AddedToken(\"<|reserved_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151692: AddedToken(\"<|reserved_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151693: AddedToken(\"<|reserved_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151694: AddedToken(\"<|reserved_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151695: AddedToken(\"<|reserved_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151696: AddedToken(\"<|reserved_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151697: AddedToken(\"<|reserved_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151698: AddedToken(\"<|reserved_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151699: AddedToken(\"<|reserved_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151700: AddedToken(\"<|reserved_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151701: AddedToken(\"<|reserved_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151702: AddedToken(\"<|reserved_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151703: AddedToken(\"<|reserved_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151704: AddedToken(\"<|reserved_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151705: AddedToken(\"<|reserved_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151706: AddedToken(\"<|reserved_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151707: AddedToken(\"<|reserved_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151708: AddedToken(\"<|reserved_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151709: AddedToken(\"<|reserved_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151710: AddedToken(\"<|reserved_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151711: AddedToken(\"<|reserved_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151712: AddedToken(\"<|reserved_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151713: AddedToken(\"<|reserved_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151714: AddedToken(\"<|reserved_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151715: AddedToken(\"<|reserved_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151716: AddedToken(\"<|reserved_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151717: AddedToken(\"<|reserved_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151718: AddedToken(\"<|reserved_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151719: AddedToken(\"<|reserved_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151720: AddedToken(\"<|reserved_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151721: AddedToken(\"<|reserved_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151722: AddedToken(\"<|reserved_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151723: AddedToken(\"<|reserved_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151724: AddedToken(\"<|reserved_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151725: AddedToken(\"<|reserved_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151726: AddedToken(\"<|reserved_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151727: AddedToken(\"<|reserved_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151728: AddedToken(\"<|reserved_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151729: AddedToken(\"<|reserved_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151730: AddedToken(\"<|reserved_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151731: AddedToken(\"<|reserved_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151732: AddedToken(\"<|reserved_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151733: AddedToken(\"<|reserved_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151734: AddedToken(\"<|reserved_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151735: AddedToken(\"<|reserved_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151736: AddedToken(\"<|reserved_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151737: AddedToken(\"<|reserved_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151738: AddedToken(\"<|reserved_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151739: AddedToken(\"<|reserved_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151740: AddedToken(\"<|reserved_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151741: AddedToken(\"<|reserved_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151742: AddedToken(\"<|reserved_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151743: AddedToken(\"<|reserved_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151744: AddedToken(\"<|reserved_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151745: AddedToken(\"<|reserved_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151746: AddedToken(\"<|reserved_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151747: AddedToken(\"<|reserved_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d19e8d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(path)\n\u001b[1;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m inputs\n",
      "File \u001b[0;32m/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/miniconda3/envs/t5gamma/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1641\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1640\u001b[0m template_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[0;32m-> 1641\u001b[0m rendered_chat, generation_indices \u001b[38;5;241m=\u001b[39m \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1653\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m rendered_chat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/miniconda3/envs/t5gamma/lib/python3.10/site-packages/transformers/utils/chat_template_utils.py:498\u001b[0m, in \u001b[0;36mrender_jinja_template\u001b[0;34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m    506\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/miniconda3/envs/t5gamma/lib/python3.10/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/miniconda3/envs/t5gamma/lib/python3.10/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:23\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(path)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Write a haiku\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(messages, add_generation_prompt=True, return_dict=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda364c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite me a poem about Machine Learning. Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:737\u001b[0m, in \u001b[0;36mT5GemmaEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, output_attentions, output_hidden_states, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     mask_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs_embeds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    734\u001b[0m     }\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     self_attn_mask_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmask_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mor_mask_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbidirectional_mask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_sliding_window_causal_mask(\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs,\n\u001b[1;32m    743\u001b[0m             or_mask_function\u001b[38;5;241m=\u001b[39msliding_window_bidirectional_mask_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msliding_window),\n\u001b[1;32m    744\u001b[0m             and_mask_function\u001b[38;5;241m=\u001b[39mbidirectional_mask_function(attention_mask),\n\u001b[1;32m    745\u001b[0m         ),\n\u001b[1;32m    746\u001b[0m     }\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/masking_utils.py:709\u001b[0m, in \u001b[0;36mcreate_causal_mask\u001b[0;34m(config, input_embeds, attention_mask, cache_position, past_key_values, or_mask_function, and_mask_function)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m or_mask_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_torch_greater_or_equal_than_2_6:\n\u001b[0;32m--> 709\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `or_mask_function` or `and_mask_function` arguments require torch>=2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m     mask_factory_function \u001b[38;5;241m=\u001b[39m or_masks(mask_factory_function, or_mask_function)\n\u001b[1;32m    711\u001b[0m     allow_is_causal_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning. Answer:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.model.encoder(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253c169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  5559,    682,    476,  19592,   1105,  13403,  14715, 235265,  10358,\n",
       "         235292]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb190683",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:737\u001b[0m, in \u001b[0;36mT5GemmaEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, output_attentions, output_hidden_states, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     mask_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs_embeds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    734\u001b[0m     }\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     self_attn_mask_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmask_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mor_mask_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbidirectional_mask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_sliding_window_causal_mask(\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmask_kwargs,\n\u001b[1;32m    743\u001b[0m             or_mask_function\u001b[38;5;241m=\u001b[39msliding_window_bidirectional_mask_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msliding_window),\n\u001b[1;32m    744\u001b[0m             and_mask_function\u001b[38;5;241m=\u001b[39mbidirectional_mask_function(attention_mask),\n\u001b[1;32m    745\u001b[0m         ),\n\u001b[1;32m    746\u001b[0m     }\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m/mmu_mllm_hdd/yangsihan05/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/masking_utils.py:709\u001b[0m, in \u001b[0;36mcreate_causal_mask\u001b[0;34m(config, input_embeds, attention_mask, cache_position, past_key_values, or_mask_function, and_mask_function)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m or_mask_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_torch_greater_or_equal_than_2_6:\n\u001b[0;32m--> 709\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `or_mask_function` or `and_mask_function` arguments require torch>=2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m     mask_factory_function \u001b[38;5;241m=\u001b[39m or_masks(mask_factory_function, or_mask_function)\n\u001b[1;32m    711\u001b[0m     allow_is_causal_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/models/Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7da9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3_last-2_2layers.yaml\n",
      "clip_last-2-2layers.yaml\n",
      "ovis2_5_last1_2layers.yaml\n",
      "qwen3_thinking_4B_last-1_2layers.yaml\n",
      "llama3_norm_avg_2layers.yaml\n",
      "qwen3vl_ours_last2_2layers.yaml\n",
      "qwen3_Base_last-2_2layers.yaml\n",
      "qwen3_4B_norm_avg_2layers.yaml\n",
      "qwen3vl_32B_norm_avg_2layers.yaml\n",
      "qwen3_32B_last-1_2layers.yaml\n",
      "ovis2_5_last2_2layers.yaml\n",
      "ovis2_5_norm_avg_2layers.yaml\n",
      "qwen3_32B_norm_avg_2layers.yaml\n",
      "qwen3_4B_last-2_2layers.yaml\n",
      "internvl3_last1_2layers-hf.yaml\n",
      "qwen3_thinking_4B_norm_avg_2layers.yaml\n",
      "qwen3vl_4B_norm_avg_2layers.yaml\n",
      "qwen3_4B_last-1_2layers.yaml\n",
      "qwen3vl_normavg_2layers.yaml\n",
      "clip_norm_avg_2layers.yaml\n",
      "xiaomi_norm_avg_2layers.yaml\n",
      "umt5xxl_norm_avg_2layers.yaml\n",
      "qwen3_Base_last-1_2layers.yaml\n",
      "qwen3vl_thinking_last_2_2layers.yaml\n",
      "qwen3vl_4B_last-2_2layers.yaml\n",
      "qwen3vl_thinking_last_1_2layers.yaml\n",
      "qwen3vl_4B_last-1_2layers.yaml\n",
      "llama3_last-1_2layers.yaml\n",
      "umt5xxl_last2_2layers.yaml\n",
      "umt5xxl_last1_2layers.yaml\n",
      "qwen3_norm_avg_2layers.yaml\n",
      "xiaomi_last1_2layers.yaml\n",
      "qwen3_thinking_4B_last-2_2layers.yaml\n",
      "qwen3vl_ours_norm_avg_2layers.yaml\n",
      "xiaomi_last2_2layers.yaml\n",
      "qwen3_32B_last-2_2layers.yaml\n",
      "qwen3_Base_norm_avg_2layers.yaml\n",
      "qwen3vl_thinking_norm_avg_2layers.yaml\n",
      "qwen3vl_ours_last1_2layers.yaml\n",
      "clip_2layers.yaml\n",
      "qwen3vl_32B_last-1_2layers.yaml\n",
      "qwen3vl_last_2_2layers.yaml\n",
      "qwen3vl_last_1_2layers.yaml\n",
      "qwen3vl_32B_last-2_2layers.yaml\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "for line in os.listdir('/ytech_m2v5_hdd/workspace/kling_mm/yangsihan05/proj_text_enc/attn_pool_contrastive/configs_paper'):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5gamma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
